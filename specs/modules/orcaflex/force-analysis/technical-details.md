# OrcaFlex Force Analysis - Technical Details

> **Module**: `orcaflex/force-analysis`  
> **Type**: Technical Implementation Specification  
> **Updated**: 2025-08-12  

## The `fe_filename` Column Discovery

### Background
The breakthrough discovery that revolutionized OrcaFlex force analysis performance was identifying the `fe_filename` column in summary files (`dm*strut_dyn.csv`). This column contains the exact simulation basename, enabling precise file matching instead of error-prone pattern matching.

### Technical Impact

#### Before Discovery: Time Series Scanning
```python
# SLOW: Scan every time series file for maximum values
def find_max_force_old_way(folder):
    max_force = -inf
    for csv_file in all_csv_files:
        df = pd.read_csv(csv_file)  # Load entire file
        current_max = df['force_column'].abs().max()  # Scan all rows
        if current_max > max_force:
            max_force = current_max
            max_file = csv_file
    # Result: 4+ hours for 1000+ files
```

#### After Discovery: Summary File Priority
```python
# FAST: Use pre-calculated max values from summary files
def find_max_force_new_way(folder):
    summary_files = glob.glob('dm*strut_dyn.csv')
    for summary_file in summary_files:
        df = pd.read_csv(summary_file)
        max_row = df.loc[df['max_columns'].abs().idxmax()]
        fe_filename = max_row['fe_filename']  # CRITICAL: Exact sim basename
        sim_basename = os.path.splitext(fe_filename)[0]
    # Result: 15 seconds for 1000+ files (1000x improvement)
```

### Data Structure Analysis

#### Summary File Structure (`dm*strut_dyn.csv`)
```
| Index | fe_filename                          | Strut1_max | Strut2_max | ... | StrutN_max |
|-------|--------------------------------------|------------|------------|-----|------------|
| 0     | fsts_l015_hwl_ncl_000deg_Jacket1.sim | 7234.56    | 8265.55    | ... | 6789.12    |
| 1     | fsts_l015_hwl_ncl_045deg_Jacket1.sim | 7892.33    | 7654.21    | ... | 7123.45    |
| ...   | ...                                  | ...        | ...        | ... | ...        |
```

**Key Insights**:
- Each row represents one complete simulation run
- `fe_filename` contains the exact `.sim` file basename
- All force columns contain pre-calculated maximum values
- No need to scan time series data for maximum identification

#### Related Time Series Files Pattern
From `fe_filename`: `fsts_l015_hwl_ncl_000deg_Jacket1.sim`  
Basename: `fsts_l015_hwl_ncl_000deg_Jacket1`

Related files:
```
fsts_l015_hwl_ncl_000deg_Jacket1.csv       # Jacket forces
fsts_l015_hwl_ncl_000deg_Strut1.csv        # Strut 1 forces  
fsts_l015_hwl_ncl_000deg_Strut2.csv        # Strut 2 forces
...
fsts_l015_hwl_ncl_000deg_Mooring1.csv      # Mooring tensions
fsts_l015_hwl_ncl_000deg_FST1_6dof_dyn.csv # Vessel motion
```

## Core Algorithm Implementation

### Maximum Force Identification Engine

```python
def process_single_strut_file(filename, base_path):
    """
    Process individual summary file to find maximum force configuration.
    
    Args:
        filename (str): Summary file name (dm*strut_dyn.csv)
        base_path (str): Base directory path
        
    Returns:
        dict: Configuration object with max force details
    """
    filepath = os.path.join(base_path, filename)
    
    try:
        # Read CSV file
        df = pd.read_csv(filepath)
        
        # Identify force columns (contain '_max' or '_min')
        force_columns = [col for col in df.columns 
                        if ('_max' in col or '_min' in col) 
                        and 'strut' in col.lower()]
        
        if not force_columns:
            return None
            
        # Find row with maximum absolute force across all columns
        max_values_per_row = df[force_columns].abs().max(axis=1)
        max_row_idx = max_values_per_row.idxmax()
        
        # Get maximum force value and column
        max_force_value = max_values_per_row.loc[max_row_idx]
        max_force_series = df[force_columns].abs().loc[max_row_idx]
        max_force_column = max_force_series.idxmax()\n        \n        # CRITICAL: Extract fe_filename from maximum force row\n        fe_filename = None\n        sim_basename = None\n        \n        if 'fe_filename' in df.columns:\n            fe_filename = df.loc[max_row_idx, 'fe_filename']\n            if pd.notna(fe_filename):\n                sim_basename = os.path.splitext(fe_filename)[0]\n        \n        # Extract configuration from filename patterns\n        configuration = extract_configuration_from_basename(sim_basename) if sim_basename else {}\n        \n        return {\n            'filename': filename,\n            'max_force': max_force_value,\n            'max_force_column': max_force_column,\n            'max_row_index': max_row_idx,\n            'fe_filename': fe_filename,\n            'sim_basename': sim_basename,\n            'configuration': configuration,\n            'force_columns_count': len(force_columns)\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error processing {filename}: {str(e)}\")\n        return None\n```\n\n### Configuration Extraction Engine\n\n```python\ndef extract_configuration_from_basename(sim_basename):\n    \"\"\"\n    Extract vessel and environment configuration from simulation basename.\n    \n    Args:\n        sim_basename (str): Simulation basename without extension\n        \n    Returns:\n        dict: Configuration parameters\n    \"\"\"\n    if not sim_basename:\n        return {}\n        \n    config = {}\n    \n    try:\n        # Split basename into components\n        parts = sim_basename.split('_')\n        \n        # Extract vessel type\n        if parts[0] in ['fsts', 'lngc']:\n            config['vessel_type'] = parts[0]\n        \n        # Extract loading conditions\n        for part in parts:\n            if part.startswith('l') and len(part) == 4 and part[1:].isdigit():\n                loading_pct = int(part[1:])\n                config['fst1'] = str(loading_pct)\n                config['fst2'] = str(loading_pct)\n                break\n        \n        # Extract tide level\n        for part in parts:\n            if part in ['hwl', 'mwl', 'lwl']:\n                config['tide'] = part\n                break\n        \n        # Extract environment type\n        for part in parts:\n            if part in ['cl', 'ncl']:\n                config['envType'] = 'colinear' if part == 'cl' else 'non-colinear'\n                break\n        \n        # Extract heading angle\n        for part in parts:\n            if part.endswith('deg') and part[:-3].isdigit():\n                config['heading'] = part[:-3]\n                break\n                \n    except Exception as e:\n        logger.warning(f\"Error extracting config from {sim_basename}: {str(e)}\")\n    \n    return config\n```\n\n### Parallel Processing Implementation\n\n```python\ndef process_strut_files_parallel(strut_files, base_path, max_workers=20):\n    \"\"\"\n    Process multiple summary files in parallel for maximum performance.\n    \n    Args:\n        strut_files (list): List of summary file names\n        base_path (str): Base directory path\n        max_workers (int): Maximum worker processes\n        \n    Returns:\n        dict: Overall maximum configuration across all files\n    \"\"\"\n    if not strut_files:\n        return None\n        \n    # Optimize worker count based on file count and system capacity\n    optimal_workers = min(max_workers, len(strut_files), os.cpu_count())\n    \n    start_time = time.time()\n    \n    with ProcessPoolExecutor(max_workers=optimal_workers) as executor:\n        # Submit all files for processing\n        futures = {executor.submit(process_single_strut_file, filename, base_path): filename \n                  for filename in strut_files}\n        \n        results = []\n        for future in as_completed(futures):\n            filename = futures[future]\n            try:\n                result = future.result()\n                if result:\n                    results.append(result)\n            except Exception as e:\n                logger.error(f\"Failed to process {filename}: {str(e)}\")\n    \n    processing_time = time.time() - start_time\n    \n    # Find overall maximum across all processed files\n    overall_max = find_overall_maximum(results)\n    \n    if overall_max:\n        overall_max['processing_time'] = processing_time\n        overall_max['files_processed'] = len(results)\n        overall_max['workers_used'] = optimal_workers\n    \n    return overall_max\n```\n\n### Pattern Matching for Parameter Changes\n\n```python\ndef modify_sim_basename_for_parameters(original_basename, parameter_changes):\n    \"\"\"\n    Modify simulation basename based on user parameter changes.\n    \n    Args:\n        original_basename (str): Original simulation basename\n        parameter_changes (dict): New parameter values\n        \n    Returns:\n        str: Modified basename for file pattern matching\n    \"\"\"\n    if not original_basename:\n        return None\n        \n    modified_basename = original_basename\n    \n    # Heading modification: 000deg -> 045deg\n    if 'heading' in parameter_changes:\n        new_heading = parameter_changes['heading']\n        heading_pattern = r'\\d{3}deg'\n        modified_basename = re.sub(heading_pattern, f\"{int(new_heading):03d}deg\", modified_basename)\n    \n    # Tide level modification: hwl -> mwl -> lwl\n    if 'tide' in parameter_changes:\n        new_tide = parameter_changes['tide']\n        tide_pattern = r'_(hwl|mwl|lwl)_'\n        modified_basename = re.sub(tide_pattern, f\"_{new_tide}_\", modified_basename)\n    \n    # Loading modification: l015 -> l095\n    if 'loading' in parameter_changes:\n        new_loading = parameter_changes['loading']\n        loading_pattern = r'_l\\d{3}_'\n        modified_basename = re.sub(loading_pattern, f\"_l{int(new_loading):03d}_\", modified_basename)\n    \n    # Environment type modification: ncl -> cl\n    if 'envType' in parameter_changes:\n        env_type = parameter_changes['envType']\n        env_code = 'cl' if env_type == 'colinear' else 'ncl'\n        env_pattern = r'_(cl|ncl)_'\n        modified_basename = re.sub(env_pattern, f\"_{env_code}_\", modified_basename)\n    \n    return modified_basename\n```\n\n## Performance Analysis\n\n### Benchmarking Results\n\n#### Processing Time Comparison\n| Method | Files | Time | Improvement |\n|--------|-------|------|-------------|\n| Time Series Scan | 952 | 4.2 hours | Baseline |\n| Summary File Scan | 952 | 15 seconds | **1008x faster** |\n| Single File Lookup | 1 | 0.02 seconds | Instant |\n\n#### Memory Usage Analysis\n```python\n# Memory-efficient processing\ndef memory_optimized_processing(files):\n    \"\"\"\n    Process files with minimal memory footprint:\n    1. Stream CSV reading (chunked)\n    2. Process one file at a time in each worker\n    3. Return only essential data (not full DataFrames)\n    4. Garbage collect after each file\n    \"\"\"\n    for filename in files:\n        try:\n            # Read only required columns\n            required_cols = ['fe_filename'] + [col for col in all_cols if '_max' in col]\n            df = pd.read_csv(filename, usecols=required_cols)\n            \n            # Process and return minimal result\n            result = process_file(df)\n            \n            # Explicit cleanup\n            del df\n            gc.collect()\n            \n            yield result\n            \n        except Exception as e:\n            logger.error(f\"Memory error processing {filename}: {e}\")\n            continue\n```\n\n#### CPU Utilization Optimization\n```python\n# Dynamic worker allocation based on system capacity\ndef calculate_optimal_workers(file_count, system_specs):\n    \"\"\"\n    Calculate optimal worker count based on:\n    1. Available CPU cores\n    2. Memory constraints  \n    3. File count and size\n    4. I/O limitations\n    \"\"\"\n    cpu_cores = os.cpu_count()\n    available_memory = psutil.virtual_memory().available\n    \n    # Memory-based limit (assume ~50MB per worker)\n    memory_workers = available_memory // (50 * 1024 * 1024)\n    \n    # CPU-based limit (leave 2 cores for system)\n    cpu_workers = max(1, cpu_cores - 2)\n    \n    # File-based limit (no more workers than files)\n    file_workers = min(file_count, 20)  # Cap at 20 for diminishing returns\n    \n    return min(memory_workers, cpu_workers, file_workers)\n```\n\n## Error Handling and Robustness\n\n### File Processing Error Handling\n```python\ndef robust_file_processing(filename, base_path):\n    \"\"\"\n    Robust file processing with comprehensive error handling.\n    \"\"\"\n    try:\n        filepath = os.path.join(base_path, filename)\n        \n        # Validate file exists and is readable\n        if not os.path.exists(filepath):\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n            \n        if not os.access(filepath, os.R_OK):\n            raise PermissionError(f\"Cannot read file: {filepath}\")\n        \n        # Check file size (empty files)\n        if os.path.getsize(filepath) == 0:\n            raise ValueError(f\"Empty file: {filepath}\")\n        \n        # Read CSV with error handling\n        try:\n            df = pd.read_csv(filepath)\n        except pd.errors.EmptyDataError:\n            raise ValueError(f\"No data in CSV: {filepath}\")\n        except pd.errors.ParserError as e:\n            raise ValueError(f\"CSV parsing error in {filepath}: {str(e)}\")\n        \n        # Validate required columns exist\n        if 'fe_filename' not in df.columns:\n            logger.warning(f\"No fe_filename column in {filename}, using fallback method\")\n            return process_file_without_fe_filename(df, filename)\n        \n        # Check for empty DataFrame\n        if df.empty:\n            raise ValueError(f\"No data rows in {filepath}\")\n        \n        # Process normally\n        return process_file_standard(df, filename)\n        \n    except Exception as e:\n        logger.error(f\"Error processing {filename}: {str(e)}\")\n        return {\n            'filename': filename,\n            'error': str(e),\n            'success': False\n        }\n```\n\n### Fallback Processing Strategy\n```python\ndef process_file_without_fe_filename(df, filename):\n    \"\"\"\n    Fallback method when fe_filename column is missing.\n    Uses filename pattern matching instead.\n    \"\"\"\n    try:\n        # Find maximum force using standard method\n        force_columns = [col for col in df.columns if '_max' in col]\n        if not force_columns:\n            return None\n            \n        max_row_idx = df[force_columns].abs().max(axis=1).idxmax()\n        max_force = df[force_columns].abs().max().max()\n        \n        # Extract configuration from summary filename instead\n        config = extract_config_from_summary_filename(filename)\n        \n        return {\n            'filename': filename,\n            'max_force': max_force,\n            'configuration': config,\n            'method': 'fallback_filename_pattern',\n            'fe_filename': None,\n            'sim_basename': None\n        }\n        \n    except Exception as e:\n        logger.error(f\"Fallback processing failed for {filename}: {str(e)}\")\n        return None\n```\n\n## File Pattern Recognition\n\n### Summary File Detection\n```python\ndef find_summary_files(directory):\n    \"\"\"\n    Intelligent detection of summary files with priority ordering.\n    \"\"\"\n    patterns = [\n        'dm*strut_dyn.csv',      # Highest priority: Strut dynamics summary\n        'dm*jacket*.csv',        # Medium priority: Jacket summary\n        '*strut_dyn.csv',        # Lower priority: Any strut dynamics\n        '*strut*.csv',           # Fallback: Any strut file\n    ]\n    \n    for pattern in patterns:\n        files = glob.glob(os.path.join(directory, pattern))\n        if files:\n            logger.info(f\"Found {len(files)} files with pattern: {pattern}\")\n            return sorted(files)  # Sort for consistent processing order\n    \n    logger.warning(f\"No summary files found in {directory}\")\n    return []\n```\n\n### Related File Discovery\n```python\ndef find_related_time_series_files(sim_basename, directory):\n    \"\"\"\n    Find all time series files related to a specific simulation basename.\n    \"\"\"\n    if not sim_basename:\n        return {}\n        \n    pattern = os.path.join(directory, f\"{sim_basename}*.csv\")\n    related_files = glob.glob(pattern)\n    \n    # Categorize files by component type\n    categorized = {\n        'jacket_forces': [],\n        'strut_forces': [],\n        'mooring_tensions': [],\n        'vessel_motion': [],\n        'other': []\n    }\n    \n    for filepath in related_files:\n        filename = os.path.basename(filepath)\n        \n        if 'jacket' in filename.lower():\n            categorized['jacket_forces'].append(filepath)\n        elif 'strut' in filename.lower():\n            categorized['strut_forces'].append(filepath)\n        elif 'mooring' in filename.lower() or 'line' in filename.lower():\n            categorized['mooring_tensions'].append(filepath)\n        elif '6dof' in filename.lower() or 'motion' in filename.lower():\n            categorized['vessel_motion'].append(filepath)\n        else:\n            categorized['other'].append(filepath)\n    \n    return categorized\n```\n\n## Integration APIs\n\n### Results Dashboard Integration\n```python\ndef get_max_force_configuration(folder_path):\n    \"\"\"\n    Main API function for Results Dashboard integration.\n    Returns complete configuration for UI auto-population.\n    \"\"\"\n    try:\n        # Find and process summary files\n        summary_files = find_summary_files(folder_path)\n        if not summary_files:\n            return {'success': False, 'error': 'No summary files found'}\n        \n        # Process files in parallel\n        max_config = process_strut_files_parallel(summary_files, folder_path)\n        if not max_config:\n            return {'success': False, 'error': 'No maximum configuration found'}\n        \n        # Find related time series files\n        if max_config.get('sim_basename'):\n            related_files = find_related_time_series_files(\n                max_config['sim_basename'], folder_path)\n            max_config['related_files'] = related_files\n        \n        return {\n            'success': True,\n            'data': max_config\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error getting max force config: {str(e)}\")\n        return {'success': False, 'error': str(e)}\n```\n\n### Browser Interface Integration  \n```python\ndef get_files_for_configuration(base_config, parameter_changes):\n    \"\"\"\n    API function for Browser Interface parameter override.\n    Returns files matching modified configuration.\n    \"\"\"\n    try:\n        if not base_config.get('sim_basename'):\n            return {'success': False, 'error': 'No base configuration provided'}\n        \n        # Modify basename based on parameter changes\n        modified_basename = modify_sim_basename_for_parameters(\n            base_config['sim_basename'], parameter_changes)\n        \n        if not modified_basename:\n            return {'success': False, 'error': 'Unable to modify configuration'}\n        \n        # Find files with modified pattern\n        folder_path = base_config.get('folder_path')\n        related_files = find_related_time_series_files(modified_basename, folder_path)\n        \n        return {\n            'success': True,\n            'data': {\n                'sim_basename': modified_basename,\n                'related_files': related_files,\n                'parameter_changes': parameter_changes\n            }\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error getting files for configuration: {str(e)}\")\n        return {'success': False, 'error': str(e)}\n```\n\n## Monitoring and Diagnostics\n\n### Performance Monitoring\n```python\nclass ForceAnalysisMonitor:\n    \"\"\"\n    Monitor performance metrics for force analysis operations.\n    \"\"\"\n    \n    def __init__(self):\n        self.metrics = {\n            'total_files_processed': 0,\n            'average_processing_time': 0,\n            'success_rate': 0,\n            'error_count': 0,\n            'cache_hit_rate': 0\n        }\n        \n    def record_operation(self, operation_result):\n        \"\"\"Record metrics from a force analysis operation.\"\"\"\n        self.metrics['total_files_processed'] += operation_result.get('files_processed', 0)\n        \n        processing_time = operation_result.get('processing_time', 0)\n        self.update_average_processing_time(processing_time)\n        \n        if operation_result.get('success', False):\n            self.metrics['success_rate'] = self.calculate_success_rate()\n        else:\n            self.metrics['error_count'] += 1\n    \n    def get_performance_report(self):\n        \"\"\"Generate comprehensive performance report.\"\"\"\n        return {\n            'metrics': self.metrics,\n            'recommendations': self.generate_recommendations(),\n            'system_health': self.assess_system_health()\n        }\n```\n\n### Diagnostic Tools\n```python\ndef diagnose_processing_issues(folder_path):\n    \"\"\"\n    Comprehensive diagnostic tool for troubleshooting processing issues.\n    \"\"\"\n    diagnostics = {\n        'folder_access': check_folder_access(folder_path),\n        'file_structure': analyze_file_structure(folder_path),\n        'summary_files': validate_summary_files(folder_path),\n        'data_quality': assess_data_quality(folder_path),\n        'system_resources': check_system_resources()\n    }\n    \n    # Generate recommendations based on diagnostics\n    recommendations = generate_diagnostic_recommendations(diagnostics)\n    \n    return {\n        'diagnostics': diagnostics,\n        'recommendations': recommendations,\n        'health_score': calculate_health_score(diagnostics)\n    }\n```\n\n---\n\n*This technical specification provides complete implementation details for the OrcaFlex Force Analysis system, focusing on the breakthrough `fe_filename` discovery and its performance implications.*