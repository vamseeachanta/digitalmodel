"""
Integration test configuration and fixtures.

Provides specialized fixtures and configuration for integration testing
of digitalmodel modules.
"""

import pytest
import sys
import os
import tempfile
import shutil
from pathlib import Path
from typing import Dict, Any, Generator
from unittest.mock import MagicMock, Mock, patch
import numpy as np
import time
import json
import yaml

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

# Import test data generator
try:
    from tests.integration.test_data.engineering_scenarios import EngineeringTestDataGenerator
except ImportError:
    # Fallback if not available
    class EngineeringTestDataGenerator:
        def __init__(self, seed=42):
            pass
        def generate_pipeline_scenarios(self):
            return {}
        def generate_plate_scenarios(self):
            return {}
        def generate_fatigue_scenarios(self):
            return {}
        def generate_reservoir_scenarios(self):
            return {}


@pytest.fixture(scope="session")
def integration_test_config():
    """Global configuration for integration tests."""
    return {
        "timeout_seconds": 300,
        "max_memory_mb": 2000,
        "performance_thresholds": {
            "pipeline_analysis": 30.0,  # seconds
            "plate_analysis": 45.0,     # seconds
            "fatigue_analysis": 20.0,   # seconds
            "memory_usage": 500.0       # MB
        },
        "test_data_seed": 42,
        "mock_external_services": True,
        "enable_performance_tracking": True
    }


@pytest.fixture(scope="session")
def engineering_test_data():
    """Generate comprehensive engineering test data for all test modules."""
    generator = EngineeringTestDataGenerator(seed=42)
    return {
        "pipeline_scenarios": generator.generate_pipeline_scenarios(),
        "plate_scenarios": generator.generate_plate_scenarios(),
        "fatigue_scenarios": generator.generate_fatigue_scenarios(),
        "reservoir_scenarios": generator.generate_reservoir_scenarios(),
        "time_series": generator.generate_time_series_data(duration_hours=24.0, sample_rate=1.0),
        "material_database": generator.generate_material_database()
    }


@pytest.fixture
def temp_directory():
    """Create temporary directory for test files."""
    temp_dir = Path(tempfile.mkdtemp())
    yield temp_dir
    shutil.rmtree(temp_dir, ignore_errors=True)


@pytest.fixture
def mock_digitalmodel_modules():
    """Mock all digitalmodel modules for integration testing."""
    mocks = {}
    
    # Mock main modules
    modules_to_mock = [
        'digitalmodel.engine',
        'digitalmodel.analysis.pipeline_analysis',
        'digitalmodel.analysis.plate_capacity',
        'digitalmodel.analysis.multi_analysis_coordinator',
        'digitalmodel.calculations.stress_analysis',
        'digitalmodel.calculations.plate_buckling',
        'digitalmodel.calculations.lateral_buckling',
        'digitalmodel.common.fatigue_analysis',
        'digitalmodel.common.parallel_processing',
        'digitalmodel.validation.template_validator',
        'digitalmodel.workflow.executor',
        'digitalmodel.migration.legacy_migrator'
    ]
    
    for module_name in modules_to_mock:
        mock_module = MagicMock()
        mocks[module_name] = mock_module
        sys.modules[module_name] = mock_module
    
    yield mocks
    
    # Cleanup
    for module_name in modules_to_mock:
        if module_name in sys.modules:
            del sys.modules[module_name]


@pytest.fixture
def performance_tracker():
    """Performance tracking utility for integration tests."""
    class IntegrationPerformanceTracker:
        def __init__(self):
            self.start_time = None
            self.measurements = {}
            self.active_measurement = None
            
        def start(self, measurement_name: str = "default"):
            """Start timing a measurement."""
            self.active_measurement = measurement_name
            self.start_time = time.time()
            
        def stop(self, measurement_name: str = None) -> float:
            """Stop timing and return elapsed time."""
            if self.start_time is None:
                return 0.0
                
            elapsed = time.time() - self.start_time
            name = measurement_name or self.active_measurement or "default"
            
            if name not in self.measurements:
                self.measurements[name] = []
            self.measurements[name].append(elapsed)
            
            self.start_time = None
            self.active_measurement = None
            return elapsed
            
        def get_average(self, measurement_name: str) -> float:
            """Get average time for a measurement."""
            times = self.measurements.get(measurement_name, [])
            return sum(times) / len(times) if times else 0.0
            
        def get_summary(self) -> Dict[str, Any]:
            """Get summary of all measurements."""
            summary = {}
            for name, times in self.measurements.items():
                summary[name] = {
                    "count": len(times),
                    "total": sum(times),
                    "average": sum(times) / len(times),
                    "min": min(times),
                    "max": max(times)
                }
            return summary
    
    return IntegrationPerformanceTracker()


@pytest.fixture
def large_test_dataset():
    """Generate large dataset for performance and stress testing."""
    np.random.seed(42)
    
    return {
        "pipeline_segments": [
            {
                "id": i,
                "diameter": 0.508 + 0.01 * np.random.randn(),
                "thickness": 0.025 + 0.002 * np.random.randn(),
                "pressure": 15e6 + 2e6 * np.random.randn(),
                "temperature": 60 + 10 * np.random.randn(),
                "x_position": i * 10.0,
                "material_grade": np.random.choice(["X65", "X70", "X80"])
            }
            for i in range(10000)
        ],
        "plate_elements": [
            {
                "element_id": i,
                "x": (i % 100) * 0.1,
                "y": (i // 100) * 0.1,
                "thickness": 0.02 + 0.001 * np.random.randn(),
                "stress_x": 200e6 + 50e6 * np.random.randn(),
                "stress_y": 180e6 + 40e6 * np.random.randn(),
                "stress_xy": 20e6 + 10e6 * np.random.randn()
            }
            for i in range(10000)
        ],
        "stress_history": {
            "time": np.linspace(0, 3600, 36000),  # 1 hour at 10 Hz
            "stress": 100e6 + 50e6 * np.sin(np.linspace(0, 20*np.pi, 36000)) + 10e6 * np.random.randn(36000)
        }
    }


@pytest.fixture
def mock_external_services():
    """Mock external services and dependencies."""
    services = {
        "database": Mock(),
        "api_service": Mock(),
        "file_system": Mock(),
        "computation_engine": Mock()
    }
    
    # Configure common mock behaviors
    services["database"].connect.return_value = True
    services["database"].query.return_value = []
    services["api_service"].get.return_value = {"status": 200, "data": {}}
    services["api_service"].post.return_value = {"status": 201, "data": {}}
    services["file_system"].exists.return_value = True
    services["file_system"].read.return_value = ""
    services["computation_engine"].solve.return_value = {"converged": True}
    
    return services


@pytest.fixture
def integration_test_markers(request):
    """Handle integration test markers and configuration."""
    markers = {}
    
    # Check for performance marker
    if request.node.get_closest_marker("performance"):
        markers["performance"] = True
        
    # Check for slow marker
    if request.node.get_closest_marker("slow"):
        markers["slow"] = True
        pytest.importorskip("time")  # Ensure time module available
        
    # Check for external marker
    if request.node.get_closest_marker("external"):
        if os.getenv("SKIP_EXTERNAL_TESTS", "false").lower() == "true":
            pytest.skip("External tests disabled")
        markers["external"] = True
        
    # Check for stress marker
    if request.node.get_closest_marker("stress"):
        if os.getenv("SKIP_STRESS_TESTS", "false").lower() == "true":
            pytest.skip("Stress tests disabled")
        markers["stress"] = True
    
    return markers


@pytest.fixture
def template_validator():
    """Mock template validator for template validation tests."""
    validator = Mock()
    
    def validate_template(template_file):
        # Simulate template validation
        return {
            "valid": True,
            "errors": [],
            "warnings": [],
            "schema_version": "2.1",
            "completeness": 1.0
        }
    
    validator.validate_pipeline_template.side_effect = validate_template
    validator.validate_plate_template.side_effect = validate_template
    validator.validate_fatigue_template.side_effect = validate_template
    
    return validator


@pytest.fixture
def workflow_executor():
    """Mock workflow executor for workflow tests."""
    executor = Mock()
    
    def execute_workflow(template=None, config=None):
        # Simulate workflow execution
        time.sleep(0.1)  # Simulate some work
        return {
            "status": "completed",
            "execution_time": 0.1,
            "stages_completed": [
                "validation", "preprocessing", "analysis", "postprocessing"
            ],
            "results": {
                "max_stress": 250e6,
                "safety_factor": 2.1,
                "utilization": 0.47
            }
        }
    
    executor.execute_workflow.side_effect = execute_workflow
    return executor


@pytest.fixture
def memory_monitor():
    """Memory usage monitoring for performance tests."""
    try:
        import psutil
        
        class MemoryMonitor:
            def __init__(self):
                self.initial_memory = None
                self.peak_memory = 0
                self.current_memory = 0
                
            def start(self):
                process = psutil.Process()
                self.initial_memory = process.memory_info().rss / 1024 / 1024  # MB
                self.peak_memory = self.initial_memory
                
            def update(self):
                process = psutil.Process()
                self.current_memory = process.memory_info().rss / 1024 / 1024  # MB
                self.peak_memory = max(self.peak_memory, self.current_memory)
                
            def get_usage(self):
                return {
                    "initial_mb": self.initial_memory,
                    "current_mb": self.current_memory,
                    "peak_mb": self.peak_memory,
                    "delta_mb": self.current_memory - self.initial_memory if self.initial_memory else 0
                }
        
        return MemoryMonitor()
    
    except ImportError:
        # Fallback if psutil not available
        class MockMemoryMonitor:
            def start(self): pass
            def update(self): pass
            def get_usage(self): return {"initial_mb": 0, "current_mb": 0, "peak_mb": 0, "delta_mb": 0}
        
        return MockMemoryMonitor()


@pytest.fixture(autouse=True)
def integration_test_setup(integration_test_config, integration_test_markers):
    """Automatic setup for all integration tests."""
    # Set timeouts based on test markers
    if integration_test_markers.get("slow"):
        pytest.timeout = integration_test_config["timeout_seconds"] * 2
    elif integration_test_markers.get("performance"):
        pytest.timeout = integration_test_config["timeout_seconds"]
    
    # Mock external dependencies if configured
    if integration_test_config["mock_external_services"]:
        with patch('digitalmodel.external.dependencies') as mock_deps:
            mock_deps.return_value = Mock()
            yield
    else:
        yield


@pytest.fixture
def test_results_collector():
    """Collect and aggregate test results across integration tests."""
    class TestResultsCollector:
        def __init__(self):
            self.results = []
            
        def add_result(self, test_name: str, status: str, duration: float, **kwargs):
            """Add a test result."""
            result = {
                "test_name": test_name,
                "status": status,
                "duration": duration,
                "timestamp": time.time(),
                **kwargs
            }
            self.results.append(result)
            
        def get_summary(self) -> Dict[str, Any]:
            """Get summary of all test results."""
            if not self.results:
                return {"total": 0, "passed": 0, "failed": 0, "average_duration": 0}
                
            total = len(self.results)
            passed = sum(1 for r in self.results if r["status"] == "passed")
            failed = total - passed
            avg_duration = sum(r["duration"] for r in self.results) / total
            
            return {
                "total": total,
                "passed": passed,
                "failed": failed,
                "success_rate": passed / total,
                "average_duration": avg_duration,
                "total_duration": sum(r["duration"] for r in self.results)
            }
            
        def save_results(self, file_path: Path):
            """Save results to file."""
            with open(file_path, 'w') as f:
                json.dump({
                    "summary": self.get_summary(),
                    "results": self.results
                }, f, indent=2)
    
    return TestResultsCollector()


# Pytest hooks for integration testing
def pytest_configure(config):
    """Configure pytest for integration testing."""
    config.addinivalue_line(
        "markers", "integration: Integration tests for module interactions"
    )
    config.addinivalue_line(
        "markers", "performance: Performance and benchmark tests"
    )
    config.addinivalue_line(
        "markers", "slow: Tests that take longer than 10 seconds"
    )
    config.addinivalue_line(
        "markers", "external: Tests requiring external services"
    )
    config.addinivalue_line(
        "markers", "template: Template validation tests"
    )
    config.addinivalue_line(
        "markers", "legacy: Legacy compatibility tests"
    )
    config.addinivalue_line(
        "markers", "workflow: End-to-end workflow tests"
    )
    config.addinivalue_line(
        "markers", "stress: Stress testing and load tests"
    )


def pytest_collection_modifyitems(config, items):
    """Modify test collection for integration testing."""
    for item in items:
        # Auto-mark integration tests
        if "integration" in str(item.fspath):
            item.add_marker(pytest.mark.integration)
            
        # Auto-mark slow tests based on patterns
        if any(keyword in item.name.lower() for keyword in ["stress", "benchmark", "large", "performance"]):
            item.add_marker(pytest.mark.slow)
            
        # Auto-mark performance tests
        if any(keyword in item.name.lower() for keyword in ["performance", "benchmark", "speed"]):
            item.add_marker(pytest.mark.performance)


def pytest_runtest_setup(item):
    """Setup for each integration test."""
    # Skip external tests in CI if configured
    if item.get_closest_marker("external") and os.getenv("CI") == "true":
        if os.getenv("SKIP_EXTERNAL_TESTS", "true").lower() == "true":
            pytest.skip("External tests disabled in CI")
            
    # Skip stress tests if configured
    if item.get_closest_marker("stress") and os.getenv("SKIP_STRESS_TESTS", "false").lower() == "true":
        pytest.skip("Stress tests disabled")


def pytest_runtest_teardown(item, nextitem):
    """Teardown for each integration test."""
    # Cleanup any temporary resources
    pass
