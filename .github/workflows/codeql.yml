name: CodeQL Analysis & Code Quality

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run CodeQL analysis weekly on Sundays at 4 AM UTC
    - cron: '0 4 * * 0'
  workflow_dispatch:
    inputs:
      analysis_type:
        description: 'Type of analysis to run'
        required: true
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'codeql'
          - 'quality'
          - 'complexity'
          - 'documentation'

env:
  PYTHON_VERSION: '3.11'

jobs:
  # CodeQL Security Analysis
  codeql-analysis:
    name: CodeQL Security Analysis
    runs-on: ubuntu-latest
    if: github.event.inputs.analysis_type == 'all' || github.event.inputs.analysis_type == 'codeql' || github.event.inputs.analysis_type == ''
    permissions:
      actions: read
      contents: read
      security-events: write
    strategy:
      fail-fast: false
      matrix:
        language: [ 'python' ]
    steps:
      - uses: actions/checkout@v4

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: ${{ matrix.language }}
          config-file: ./.github/codeql/codeql-config.yml

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies for analysis
        run: |
          uv sync --frozen --no-dev

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: "/language:${{matrix.language}}"

  # Code Quality Analysis
  code-quality:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    if: github.event.inputs.analysis_type == 'all' || github.event.inputs.analysis_type == 'quality' || github.event.inputs.analysis_type == ''
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install quality analysis tools
        run: |
          uv sync --frozen
          uv pip install \
            ruff \
            black \
            isort \
            mypy \
            pylint \
            flake8 \
            vulture \
            pydocstyle \
            interrogate \
            safety

      - name: Cache quality tools
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/ruff
            ~/.cache/mypy
            ~/.cache/pylint
          key: quality-cache-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            quality-cache-${{ runner.os }}-

      - name: Run Ruff (linting and formatting)
        run: |
          echo "Running Ruff linter..."
          uv run ruff check \
            --output-format=github \
            --statistics \
            --show-fixes \
            src/ > ruff-report.txt || true

          echo "Running Ruff formatter check..."
          uv run ruff format \
            --check \
            --diff \
            src/ >> ruff-report.txt || true

      - name: Run Black formatting check
        run: |
          echo "Running Black formatter check..."
          uv run black \
            --check \
            --diff \
            --color \
            src/ > black-report.txt || true

      - name: Run isort import sorting check
        run: |
          echo "Running isort import check..."
          uv run isort \
            --check-only \
            --diff \
            --color \
            src/ > isort-report.txt || true

      - name: Run MyPy type checking
        run: |
          echo "Running MyPy type checking..."
          uv run mypy \
            src/ \
            --html-report mypy-report \
            --txt-report mypy-txt-report \
            --show-error-codes \
            --pretty || true

      - name: Run Pylint analysis
        run: |
          echo "Running Pylint analysis..."
          uv run pylint \
            src/ \
            --output-format=json:pylint-report.json,text:pylint-report.txt \
            --score=yes \
            --reports=yes || true

      - name: Run Flake8 analysis
        run: |
          echo "Running Flake8 analysis..."
          uv run flake8 \
            src/ \
            --format=json \
            --output-file=flake8-report.json \
            --statistics \
            --count || true

      - name: Run Vulture (dead code detection)
        run: |
          echo "Running Vulture dead code detection..."
          uv run vulture \
            src/ \
            --min-confidence 80 \
            --sort-by-size > vulture-report.txt || true

      - name: Run pydocstyle (docstring analysis)
        run: |
          echo "Running pydocstyle docstring analysis..."
          uv run pydocstyle \
            src/ \
            --explain \
            --source > pydocstyle-report.txt || true

      - name: Generate quality metrics summary
        run: |
          python -c "
          import json
          import re
          from pathlib import Path
          from collections import defaultdict

          metrics = defaultdict(int)
          issues = defaultdict(list)

          # Process Pylint results
          if Path('pylint-report.json').exists():
              with open('pylint-report.json') as f:
                  pylint_data = json.load(f)

              for item in pylint_data:
                  issue_type = item.get('type', 'unknown')
                  metrics[f'pylint_{issue_type}'] += 1
                  issues['pylint'].append({
                      'type': issue_type,
                      'message': item.get('message', ''),
                      'file': item.get('path', ''),
                      'line': item.get('line', 0)
                  })

          # Process MyPy results
          mypy_errors = 0
          if Path('mypy-txt-report/index.txt').exists():
              with open('mypy-txt-report/index.txt') as f:
                  content = f.read()
                  error_match = re.search(r'(\d+) error', content)
                  if error_match:
                      mypy_errors = int(error_match.group(1))

          metrics['mypy_errors'] = mypy_errors

          # Process Flake8 results
          if Path('flake8-report.json').exists():
              with open('flake8-report.json') as f:
                  flake8_data = json.load(f)
              metrics['flake8_issues'] = len(flake8_data)

          # Count lines of code
          total_lines = 0
          total_files = 0

          for py_file in Path('src').rglob('*.py'):
              total_files += 1
              with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                  total_lines += len(f.readlines())

          metrics['total_files'] = total_files
          metrics['total_lines'] = total_lines

          # Generate summary report
          with open('quality-summary.md', 'w') as f:
              f.write('# ðŸ“Š Code Quality Analysis Report\\n\\n')

              f.write('## ðŸ“ˆ Metrics Overview\\n\\n')
              f.write('| Metric | Value |\\n')
              f.write('|--------|-------|\\n')
              f.write(f'| Total Files | {metrics[\"total_files\"]} |\\n')
              f.write(f'| Total Lines of Code | {metrics[\"total_lines\"]:,} |\\n')
              f.write(f'| MyPy Errors | {metrics[\"mypy_errors\"]} |\\n')
              f.write(f'| Flake8 Issues | {metrics[\"flake8_issues\"]} |\\n')
              f.write(f'| Pylint Errors | {metrics[\"pylint_error\"]} |\\n')
              f.write(f'| Pylint Warnings | {metrics[\"pylint_warning\"]} |\\n')
              f.write(f'| Pylint Conventions | {metrics[\"pylint_convention\"]} |\\n')
              f.write(f'| Pylint Refactors | {metrics[\"pylint_refactor\"]} |\\n')

              # Quality score calculation
              total_issues = (metrics['mypy_errors'] +
                            metrics['flake8_issues'] +
                            metrics['pylint_error'] +
                            metrics['pylint_warning'])

              if metrics['total_lines'] > 0:
                  quality_score = max(0, 100 - (total_issues * 100 / metrics['total_lines']))
              else:
                  quality_score = 100

              f.write(f'\\n## ðŸŽ¯ Quality Score: {quality_score:.1f}/100\\n\\n')

              if quality_score >= 90:
                  f.write('âœ… **Excellent** - Code quality is very high\\n')
              elif quality_score >= 80:
                  f.write('ðŸŸ¡ **Good** - Code quality is acceptable with room for improvement\\n')
              elif quality_score >= 70:
                  f.write('ðŸŸ  **Fair** - Code quality needs attention\\n')
              else:
                  f.write('ðŸ”´ **Poor** - Code quality requires immediate attention\\n')

              f.write(f'\\n## ðŸ“‹ Tool Reports\\n\\n')
              f.write('Detailed reports are available in the workflow artifacts:\\n')
              f.write('- Ruff: `ruff-report.txt`\\n')
              f.write('- Black: `black-report.txt`\\n')
              f.write('- isort: `isort-report.txt`\\n')
              f.write('- MyPy: `mypy-report/` (HTML) and `mypy-txt-report/` (text)\\n')
              f.write('- Pylint: `pylint-report.json` and `pylint-report.txt`\\n')
              f.write('- Flake8: `flake8-report.json`\\n')
              f.write('- Vulture: `vulture-report.txt`\\n')
              f.write('- pydocstyle: `pydocstyle-report.txt`\\n')

          # Save metrics as JSON for other tools
          with open('quality-metrics.json', 'w') as f:
              json.dump(dict(metrics), f, indent=2)

          print(f'Quality analysis complete. Score: {quality_score:.1f}/100')
          "

      - name: Upload quality analysis results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: code-quality-analysis
          path: |
            quality-summary.md
            quality-metrics.json
            ruff-report.txt
            black-report.txt
            isort-report.txt
            mypy-report/
            mypy-txt-report/
            pylint-report.json
            pylint-report.txt
            flake8-report.json
            vulture-report.txt
            pydocstyle-report.txt

  # Complexity Analysis
  complexity-analysis:
    name: Complexity Analysis
    runs-on: ubuntu-latest
    if: github.event.inputs.analysis_type == 'all' || github.event.inputs.analysis_type == 'complexity' || github.event.inputs.analysis_type == ''
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install complexity analysis tools
        run: |
          uv pip install \
            radon \
            xenon \
            mccabe \
            cohesion

      - name: Run Radon complexity analysis
        run: |
          echo "Running Radon complexity analysis..."

          # Cyclomatic complexity
          uv run radon cc src/ \
            --json \
            --output-file radon-cc.json

          uv run radon cc src/ \
            --show-complexity \
            --average > radon-cc-report.txt

          # Maintainability index
          uv run radon mi src/ \
            --json \
            --output-file radon-mi.json

          uv run radon mi src/ \
            --show > radon-mi-report.txt

          # Raw metrics
          uv run radon raw src/ \
            --json \
            --output-file radon-raw.json

          uv run radon raw src/ \
            --summary > radon-raw-report.txt

          # Halstead metrics
          uv run radon hal src/ \
            --json \
            --output-file radon-hal.json

          uv run radon hal src/ > radon-hal-report.txt

      - name: Run Xenon complexity analysis
        run: |
          echo "Running Xenon complexity analysis..."
          uv run xenon \
            --max-absolute B \
            --max-modules A \
            --max-average A \
            src/ > xenon-report.txt || true

      - name: Run cohesion analysis
        run: |
          echo "Running cohesion analysis..."
          uv run cohesion \
            --directory src/ \
            --verbose > cohesion-report.txt || true

      - name: Generate complexity summary
        run: |
          python -c "
          import json
          import statistics
          from pathlib import Path
          from collections import defaultdict

          complexity_data = defaultdict(list)
          maintainability_data = []

          # Process Radon cyclomatic complexity
          if Path('radon-cc.json').exists():
              with open('radon-cc.json') as f:
                  cc_data = json.load(f)

              for file_path, functions in cc_data.items():
                  for func_data in functions:
                      complexity = func_data.get('complexity', 0)
                      complexity_data['cyclomatic'].append(complexity)

          # Process Radon maintainability index
          if Path('radon-mi.json').exists():
              with open('radon-mi.json') as f:
                  mi_data = json.load(f)

              for file_path, mi_value in mi_data.items():
                  if isinstance(mi_value, (int, float)):
                      maintainability_data.append(mi_value)

          # Calculate statistics
          cc_values = complexity_data['cyclomatic']
          if cc_values:
              avg_complexity = statistics.mean(cc_values)
              max_complexity = max(cc_values)
              complex_functions = len([x for x in cc_values if x > 10])
          else:
              avg_complexity = max_complexity = complex_functions = 0

          if maintainability_data:
              avg_maintainability = statistics.mean(maintainability_data)
              min_maintainability = min(maintainability_data)
          else:
              avg_maintainability = min_maintainability = 0

          # Generate summary report
          with open('complexity-summary.md', 'w') as f:
              f.write('# ðŸ§® Complexity Analysis Report\\n\\n')

              f.write('## ðŸ“Š Complexity Metrics\\n\\n')
              f.write('| Metric | Value |\\n')
              f.write('|--------|-------|\\n')
              f.write(f'| Average Cyclomatic Complexity | {avg_complexity:.2f} |\\n')
              f.write(f'| Maximum Cyclomatic Complexity | {max_complexity} |\\n')
              f.write(f'| Functions with High Complexity (>10) | {complex_functions} |\\n')
              f.write(f'| Average Maintainability Index | {avg_maintainability:.2f} |\\n')
              f.write(f'| Lowest Maintainability Index | {min_maintainability:.2f} |\\n')

              f.write('\\n## ðŸŽ¯ Complexity Assessment\\n\\n')

              if avg_complexity <= 5:
                  f.write('âœ… **Low Complexity** - Code is easy to understand and maintain\\n')
              elif avg_complexity <= 10:
                  f.write('ðŸŸ¡ **Moderate Complexity** - Code complexity is acceptable\\n')
              elif avg_complexity <= 15:
                  f.write('ðŸŸ  **High Complexity** - Consider refactoring complex functions\\n')
              else:
                  f.write('ðŸ”´ **Very High Complexity** - Immediate refactoring recommended\\n')

              if avg_maintainability >= 85:
                  f.write('âœ… **Highly Maintainable** - Code is very easy to maintain\\n')
              elif avg_maintainability >= 70:
                  f.write('ðŸŸ¡ **Moderately Maintainable** - Code maintainability is acceptable\\n')
              elif avg_maintainability >= 50:
                  f.write('ðŸŸ  **Low Maintainability** - Consider refactoring for better maintainability\\n')
              else:
                  f.write('ðŸ”´ **Very Low Maintainability** - Significant refactoring needed\\n')

              f.write('\\n## ðŸ“‹ Detailed Reports\\n\\n')
              f.write('- Cyclomatic Complexity: `radon-cc-report.txt`\\n')
              f.write('- Maintainability Index: `radon-mi-report.txt`\\n')
              f.write('- Raw Metrics: `radon-raw-report.txt`\\n')
              f.write('- Halstead Metrics: `radon-hal-report.txt`\\n')
              f.write('- Xenon Analysis: `xenon-report.txt`\\n')
              f.write('- Cohesion Analysis: `cohesion-report.txt`\\n')

          # Save metrics for other tools
          metrics = {
              'average_complexity': avg_complexity,
              'max_complexity': max_complexity,
              'complex_functions': complex_functions,
              'average_maintainability': avg_maintainability,
              'min_maintainability': min_maintainability,
              'total_functions': len(cc_values)
          }

          with open('complexity-metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)

          print(f'Complexity analysis complete. Average complexity: {avg_complexity:.2f}')
          "

      - name: Upload complexity analysis results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: complexity-analysis
          path: |
            complexity-summary.md
            complexity-metrics.json
            radon-*.json
            radon-*-report.txt
            xenon-report.txt
            cohesion-report.txt

  # Documentation Coverage
  documentation-coverage:
    name: Documentation Coverage
    runs-on: ubuntu-latest
    if: github.event.inputs.analysis_type == 'all' || github.event.inputs.analysis_type == 'documentation' || github.event.inputs.analysis_type == ''
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install documentation tools
        run: |
          uv pip install \
            interrogate \
            pydoc-markdown \
            docstring-coverage

      - name: Run documentation coverage analysis
        run: |
          echo "Running documentation coverage analysis..."

          # Interrogate docstring coverage
          uv run interrogate \
            --verbose \
            --ignore-init-method \
            --ignore-init-module \
            --ignore-magic \
            --ignore-nested-functions \
            --ignore-semiprivate \
            --ignore-private \
            --fail-under=80 \
            --print-summary \
            --output interrogate-report.txt \
            src/ || true

          # Generate badge data
          uv run interrogate \
            --generate-badge interrogate-badge.svg \
            src/

          # Docstring coverage alternative tool
          uv run docstring-coverage \
            src/ \
            --verbose \
            --badge=docstring-coverage-badge.svg > docstring-coverage-report.txt || true

      - name: Analyze documentation quality
        run: |
          python -c "
          import re
          import json
          from pathlib import Path

          # Parse interrogate output
          doc_coverage = 0
          if Path('interrogate-report.txt').exists():
              with open('interrogate-report.txt') as f:
                  content = f.read()

              # Extract coverage percentage
              coverage_match = re.search(r'TOTAL.*?(\d+\.\d+)%', content)
              if coverage_match:
                  doc_coverage = float(coverage_match.group(1))

          # Count documentation files
          doc_files = list(Path('docs').rglob('*.md')) if Path('docs').exists() else []
          readme_exists = Path('README.md').exists()

          # Count docstrings in source
          total_functions = 0
          documented_functions = 0

          for py_file in Path('src').rglob('*.py'):
              with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                  content = f.read()

              # Simple function counting (not perfect but indicative)
              functions = re.findall(r'^def \\w+\\(', content, re.MULTILINE)
              classes = re.findall(r'^class \\w+', content, re.MULTILINE)

              total_functions += len(functions) + len(classes)

              # Count functions/classes with docstrings
              # This is a simplified check
              for match in re.finditer(r'^(def|class) \\w+.*?:\\s*\"\"\"', content, re.MULTILINE | re.DOTALL):
                  documented_functions += 1

          # Generate summary
          with open('documentation-summary.md', 'w') as f:
              f.write('# ðŸ“š Documentation Coverage Report\\n\\n')

              f.write('## ðŸ“Š Coverage Metrics\\n\\n')
              f.write('| Metric | Value |\\n')
              f.write('|--------|-------|\\n')
              f.write(f'| Docstring Coverage | {doc_coverage:.1f}% |\\n')
              f.write(f'| Total Functions/Classes | {total_functions} |\\n')
              f.write(f'| Documented Functions/Classes | {documented_functions} |\\n')
              f.write(f'| Documentation Files | {len(doc_files)} |\\n')
              f.write(f'| README Present | {\"Yes\" if readme_exists else \"No\"} |\\n')

              f.write('\\n## ðŸŽ¯ Documentation Assessment\\n\\n')

              if doc_coverage >= 90:
                  f.write('âœ… **Excellent** - Documentation coverage is very high\\n')
              elif doc_coverage >= 80:
                  f.write('ðŸŸ¡ **Good** - Documentation coverage is acceptable\\n')
              elif doc_coverage >= 60:
                  f.write('ðŸŸ  **Fair** - Documentation coverage needs improvement\\n')
              else:
                  f.write('ðŸ”´ **Poor** - Documentation coverage requires immediate attention\\n')

              f.write('\\n## ðŸ“‹ Detailed Reports\\n\\n')
              f.write('- Interrogate Analysis: `interrogate-report.txt`\\n')
              f.write('- Docstring Coverage: `docstring-coverage-report.txt`\\n')
              f.write('- Coverage Badges: `*-badge.svg`\\n')

          # Save metrics
          metrics = {
              'docstring_coverage': doc_coverage,
              'total_functions': total_functions,
              'documented_functions': documented_functions,
              'documentation_files': len(doc_files),
              'readme_exists': readme_exists
          }

          with open('documentation-metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)

          print(f'Documentation analysis complete. Coverage: {doc_coverage:.1f}%')
          "

      - name: Upload documentation analysis results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: documentation-analysis
          path: |
            documentation-summary.md
            documentation-metrics.json
            interrogate-report.txt
            docstring-coverage-report.txt
            *-badge.svg

  # Comprehensive analysis summary
  analysis-summary:
    name: Analysis Summary
    runs-on: ubuntu-latest
    needs: [codeql-analysis, code-quality, complexity-analysis, documentation-coverage]
    if: always()
    steps:
      - uses: actions/checkout@v4

      - name: Download all analysis results
        uses: actions/download-artifact@v4
        with:
          path: analysis-results/

      - name: Generate comprehensive summary
        run: |
          python -c "
          import json
          import os
          from pathlib import Path
          from datetime import datetime

          # Collect all metrics
          all_metrics = {
              'analysis_date': datetime.now().isoformat(),
              'repository': '${{ github.repository }}',
              'branch': '${{ github.ref_name }}',
              'commit': '${{ github.sha }}',
              'quality': {},
              'complexity': {},
              'documentation': {}
          }

          # Load quality metrics
          quality_file = Path('analysis-results/code-quality-analysis/quality-metrics.json')
          if quality_file.exists():
              with open(quality_file) as f:
                  all_metrics['quality'] = json.load(f)

          # Load complexity metrics
          complexity_file = Path('analysis-results/complexity-analysis/complexity-metrics.json')
          if complexity_file.exists():
              with open(complexity_file) as f:
                  all_metrics['complexity'] = json.load(f)

          # Load documentation metrics
          doc_file = Path('analysis-results/documentation-analysis/documentation-metrics.json')
          if doc_file.exists():
              with open(doc_file) as f:
                  all_metrics['documentation'] = json.load(f)

          # Generate comprehensive report
          with open('comprehensive-analysis.md', 'w') as f:
              f.write('# ðŸ” Comprehensive Code Analysis Report\\n\\n')
              f.write(f'**Repository:** {all_metrics[\"repository\"]}\\n')
              f.write(f'**Branch:** {all_metrics[\"branch\"]}\\n')
              f.write(f'**Commit:** {all_metrics[\"commit\"][:8]}\\n')
              f.write(f'**Analysis Date:** {all_metrics[\"analysis_date\"]}\\n\\n')

              f.write('## ðŸŽ¯ Executive Summary\\n\\n')

              # Calculate overall health score
              scores = []

              if 'quality' in all_metrics and all_metrics['quality']:
                  total_issues = (all_metrics['quality'].get('mypy_errors', 0) +
                                all_metrics['quality'].get('flake8_issues', 0) +
                                all_metrics['quality'].get('pylint_error', 0))
                  total_lines = all_metrics['quality'].get('total_lines', 1)
                  quality_score = max(0, 100 - (total_issues * 100 / total_lines))
                  scores.append(quality_score)
                  f.write(f'- **Code Quality Score:** {quality_score:.1f}/100\\n')

              if 'complexity' in all_metrics and all_metrics['complexity']:
                  avg_complexity = all_metrics['complexity'].get('average_complexity', 0)
                  complexity_score = max(0, 100 - (avg_complexity * 5))  # Scale complexity to 0-100
                  scores.append(complexity_score)
                  f.write(f'- **Complexity Score:** {complexity_score:.1f}/100\\n')

              if 'documentation' in all_metrics and all_metrics['documentation']:
                  doc_coverage = all_metrics['documentation'].get('docstring_coverage', 0)
                  scores.append(doc_coverage)
                  f.write(f'- **Documentation Score:** {doc_coverage:.1f}/100\\n')

              if scores:
                  overall_score = sum(scores) / len(scores)
                  f.write(f'\\n**Overall Health Score: {overall_score:.1f}/100**\\n\\n')

                  if overall_score >= 85:
                      f.write('ðŸŸ¢ **Excellent** - Codebase is in excellent condition\\n')
                  elif overall_score >= 70:
                      f.write('ðŸŸ¡ **Good** - Codebase quality is good with minor improvements needed\\n')
                  elif overall_score >= 50:
                      f.write('ðŸŸ  **Fair** - Codebase needs attention in several areas\\n')
                  else:
                      f.write('ðŸ”´ **Poor** - Codebase requires significant improvements\\n')

              f.write('\\n## ðŸ“Š Detailed Metrics\\n\\n')

              # Quality section
              if all_metrics['quality']:
                  f.write('### Code Quality\\n\\n')
                  q = all_metrics['quality']
                  f.write(f'- Total Files: {q.get(\"total_files\", 0)}\\n')
                  f.write(f'- Lines of Code: {q.get(\"total_lines\", 0):,}\\n')
                  f.write(f'- MyPy Errors: {q.get(\"mypy_errors\", 0)}\\n')
                  f.write(f'- Pylint Issues: {q.get(\"pylint_error\", 0) + q.get(\"pylint_warning\", 0)}\\n')
                  f.write(f'- Flake8 Issues: {q.get(\"flake8_issues\", 0)}\\n\\n')

              # Complexity section
              if all_metrics['complexity']:
                  f.write('### Code Complexity\\n\\n')
                  c = all_metrics['complexity']
                  f.write(f'- Average Cyclomatic Complexity: {c.get(\"average_complexity\", 0):.2f}\\n')
                  f.write(f'- Maximum Complexity: {c.get(\"max_complexity\", 0)}\\n')
                  f.write(f'- High Complexity Functions: {c.get(\"complex_functions\", 0)}\\n')
                  f.write(f'- Maintainability Index: {c.get(\"average_maintainability\", 0):.2f}\\n\\n')

              # Documentation section
              if all_metrics['documentation']:
                  f.write('### Documentation\\n\\n')
                  d = all_metrics['documentation']
                  f.write(f'- Docstring Coverage: {d.get(\"docstring_coverage\", 0):.1f}%\\n')
                  f.write(f'- Documented Functions: {d.get(\"documented_functions\", 0)}\\n')
                  f.write(f'- Total Functions: {d.get(\"total_functions\", 0)}\\n')
                  f.write(f'- Documentation Files: {d.get(\"documentation_files\", 0)}\\n\\n')

              f.write('## ðŸ›  Recommendations\\n\\n')

              # Generate recommendations based on metrics
              if all_metrics['quality'].get('mypy_errors', 0) > 0:
                  f.write('- ðŸ”§ Fix MyPy type errors for better type safety\\n')

              if all_metrics['complexity'].get('average_complexity', 0) > 10:
                  f.write('- ðŸ”§ Refactor high-complexity functions\\n')

              if all_metrics['documentation'].get('docstring_coverage', 0) < 80:
                  f.write('- ðŸ“š Improve documentation coverage\\n')

              f.write('\\n## ðŸ“‹ Available Reports\\n\\n')
              f.write('Detailed analysis results are available in the workflow artifacts.\\n')

          # Save comprehensive metrics
          with open('comprehensive-metrics.json', 'w') as f:
              json.dump(all_metrics, f, indent=2)

          print('Comprehensive analysis summary generated')
          "

      - name: Upload comprehensive analysis
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-analysis
          path: |
            comprehensive-analysis.md
            comprehensive-metrics.json

      - name: Post analysis summary to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            try {
              const summary = fs.readFileSync('comprehensive-analysis.md', 'utf8');

              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summary.substring(0, 65000) // GitHub comment limit
              });
            } catch (error) {
              console.log('Could not post analysis summary:', error.message);

              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: 'ðŸ” **Code Analysis Completed**\n\nComprehensive code analysis results are available in the workflow artifacts.'
              });
            }