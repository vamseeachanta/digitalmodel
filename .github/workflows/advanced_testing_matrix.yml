name: Advanced Testing Matrix - Gold Standard

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run comprehensive tests nightly
    - cron: '0 2 * * *'

env:
  PYTHON_DEFAULT: "3.11"
  UV_SYSTEM_PYTHON: 1

jobs:
  # Test matrix across multiple Python versions and platforms
  test-matrix:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ["3.9", "3.10", "3.11"]
        test-type: [unit, integration, security]
        exclude:
          # Skip some combinations to optimize CI time
          - os: macos-latest
            python-version: "3.9"
          - os: windows-latest
            python-version: "3.9"

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Install dependencies
      run: |
        uv pip install --system -e .

    - name: Create reports directory
      run: mkdir -p reports

    - name: Run ${{ matrix.test-type }} tests
      run: |
        pytest -m ${{ matrix.test-type }} \
          --cov=src \
          --cov-report=xml:reports/coverage-${{ matrix.os }}-${{ matrix.python-version }}-${{ matrix.test-type }}.xml \
          --cov-report=html:reports/htmlcov-${{ matrix.os }}-${{ matrix.python-version }}-${{ matrix.test-type }} \
          --html=reports/pytest-${{ matrix.os }}-${{ matrix.python-version }}-${{ matrix.test-type }}.html \
          --json-report --json-report-file=reports/test-${{ matrix.os }}-${{ matrix.python-version }}-${{ matrix.test-type }}.json \
          --tb=short \
          -v

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.os }}-${{ matrix.python-version }}-${{ matrix.test-type }}
        path: reports/

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: reports/coverage-${{ matrix.os }}-${{ matrix.python-version }}-${{ matrix.test-type }}.xml
        flags: ${{ matrix.test-type }}-${{ matrix.os }}-py${{ matrix.python-version }}
        name: ${{ matrix.os }}-${{ matrix.python-version }}-${{ matrix.test-type }}

  # Performance benchmarking
  performance-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}

    - name: Install dependencies
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        uv pip install --system -e .

    - name: Run performance benchmarks
      run: |
        pytest -m benchmark \
          --benchmark-json=reports/benchmarks.json \
          --benchmark-compare-fail=mean:10% \
          --benchmark-histogram=reports/benchmark-histogram

    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: reports/benchmarks.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '110%'
        fail-on-alert: true

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: reports/

  # Property-based testing with different profiles
  property-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        hypothesis-profile: [dev, ci, thorough]
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}

    - name: Install dependencies
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        uv pip install --system -e .

    - name: Run property-based tests
      env:
        HYPOTHESIS_PROFILE: ${{ matrix.hypothesis-profile }}
      run: |
        pytest -m property \
          --html=reports/property-${{ matrix.hypothesis-profile }}.html \
          --json-report --json-report-file=reports/property-${{ matrix.hypothesis-profile }}.json \
          -v

    - name: Upload property test results
      uses: actions/upload-artifact@v3
      with:
        name: property-test-results-${{ matrix.hypothesis-profile }}
        path: reports/

  # Mutation testing (nightly or on demand)
  mutation-tests:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[mutation]')
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}

    - name: Install dependencies
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        uv pip install --system -e .

    - name: Run mutation testing
      run: |
        mutmut run --paths-to-mutate=src/ --tests-dir=tests/
        mutmut results > reports/mutation-results.txt
        mutmut json-report > reports/mutation-report.json

    - name: Generate mutation report
      run: |
        echo "## Mutation Testing Results" >> $GITHUB_STEP_SUMMARY
        echo "### Summary" >> $GITHUB_STEP_SUMMARY
        mutmut results >> $GITHUB_STEP_SUMMARY

    - name: Upload mutation results
      uses: actions/upload-artifact@v3
      with:
        name: mutation-test-results
        path: reports/

    - name: Comment mutation results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const results = fs.readFileSync('reports/mutation-results.txt', 'utf8');
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## ðŸ§¬ Mutation Testing Results\n\n\`\`\`\n${results}\n\`\`\``
          });

  # Security scanning
  security-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}

    - name: Install dependencies
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        uv pip install --system -e .
        uv pip install --system bandit safety

    - name: Run security tests
      run: |
        pytest -m security \
          --html=reports/security-tests.html \
          --json-report --json-report-file=reports/security-tests.json

    - name: Run Bandit security scan
      run: |
        bandit -r src/ -f json -o reports/bandit-report.json
        bandit -r src/ -f txt -o reports/bandit-report.txt

    - name: Run Safety dependency check
      run: |
        safety check --json --output reports/safety-report.json || true
        safety check --output reports/safety-report.txt || true

    - name: Upload security results
      uses: actions/upload-artifact@v3
      with:
        name: security-scan-results
        path: reports/

  # Parallel test execution optimization
  parallel-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}

    - name: Install dependencies
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        uv pip install --system -e .

    - name: Run tests with different parallelization
      run: |
        echo "## Parallel Test Performance" >> $GITHUB_STEP_SUMMARY

        # Sequential
        echo "### Sequential Execution" >> $GITHUB_STEP_SUMMARY
        time pytest -m unit --tb=no -q | tee sequential.log

        # Parallel with auto workers
        echo "### Parallel Execution (auto)" >> $GITHUB_STEP_SUMMARY
        time pytest -m unit -n auto --tb=no -q | tee parallel-auto.log

        # Parallel with specific worker count
        echo "### Parallel Execution (4 workers)" >> $GITHUB_STEP_SUMMARY
        time pytest -m unit -n 4 --tb=no -q | tee parallel-4.log

    - name: Analyze parallel performance
      run: |
        echo "### Performance Analysis" >> $GITHUB_STEP_SUMMARY
        echo "Sequential time: $(grep -o '[0-9.]*s' sequential.log | tail -1)" >> $GITHUB_STEP_SUMMARY
        echo "Parallel auto time: $(grep -o '[0-9.]*s' parallel-auto.log | tail -1)" >> $GITHUB_STEP_SUMMARY
        echo "Parallel 4 workers time: $(grep -o '[0-9.]*s' parallel-4.log | tail -1)" >> $GITHUB_STEP_SUMMARY

  # Test quality metrics and reporting
  quality-metrics:
    runs-on: ubuntu-latest
    needs: [test-matrix, performance-tests, property-tests]
    if: always()
    steps:
    - uses: actions/checkout@v4

    - name: Download all test artifacts
      uses: actions/download-artifact@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}

    - name: Install analysis tools
      run: |
        pip install jq yq pytest-json-report

    - name: Generate comprehensive test report
      run: |
        echo "# ðŸ“Š Test Quality Dashboard" > test-dashboard.md
        echo "" >> test-dashboard.md
        echo "## Test Execution Summary" >> test-dashboard.md
        echo "- **Total Test Runs**: $(find . -name 'test-*.json' | wc -l)" >> test-dashboard.md
        echo "- **Platforms Tested**: Linux, macOS, Windows" >> test-dashboard.md
        echo "- **Python Versions**: 3.9, 3.10, 3.11" >> test-dashboard.md
        echo "" >> test-dashboard.md

    - name: Analyze coverage trends
      run: |
        echo "## Coverage Analysis" >> test-dashboard.md
        for coverage_file in $(find . -name 'coverage-*.xml'); do
          platform=$(echo $coverage_file | cut -d'-' -f2)
          python_ver=$(echo $coverage_file | cut -d'-' -f3)
          test_type=$(echo $coverage_file | cut -d'-' -f4 | cut -d'.' -f1)
          coverage=$(grep -o 'line-rate="[0-9.]*"' $coverage_file | cut -d'"' -f2 | head -1)
          coverage_percent=$(echo "$coverage * 100" | bc -l | cut -d'.' -f1)
          echo "- **$platform-$python_ver-$test_type**: ${coverage_percent}%" >> test-dashboard.md
        done

    - name: Performance metrics analysis
      run: |
        if [ -f benchmark-results/benchmarks.json ]; then
          echo "" >> test-dashboard.md
          echo "## Performance Metrics" >> test-dashboard.md
          echo "- **Benchmark Results**: Available in artifacts" >> test-dashboard.md
          echo "- **Performance Regression**: $(jq -r '.machine_info.cpu_count' benchmark-results/benchmarks.json) cores tested" >> test-dashboard.md
        fi

    - name: Create summary report
      run: |
        echo "" >> test-dashboard.md
        echo "## Quality Gates" >> test-dashboard.md
        echo "- âœ… Multi-platform testing" >> test-dashboard.md
        echo "- âœ… Multi-Python version support" >> test-dashboard.md
        echo "- âœ… Security testing enabled" >> test-dashboard.md
        echo "- âœ… Performance benchmarking" >> test-dashboard.md
        echo "- âœ… Property-based testing" >> test-dashboard.md
        echo "" >> test-dashboard.md
        echo "Generated on: $(date)" >> test-dashboard.md

    - name: Upload quality dashboard
      uses: actions/upload-artifact@v3
      with:
        name: quality-dashboard
        path: test-dashboard.md

    - name: Comment dashboard on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const dashboard = fs.readFileSync('test-dashboard.md', 'utf8');
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: dashboard
          });

  # Test flakiness detection
  flaky-test-detection:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}

    - name: Install dependencies
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        uv pip install --system -e .
        uv pip install --system pytest-rerunfailures

    - name: Run flaky test detection
      run: |
        # Run tests multiple times to detect flakiness
        for i in {1..10}; do
          echo "Run $i:"
          pytest --tb=line -q --json-report --json-report-file=reports/flaky-run-$i.json || true
        done

    - name: Analyze flaky tests
      run: |
        echo "# ðŸŽ² Flaky Test Detection Report" > flaky-report.md
        echo "" >> flaky-report.md
        echo "Executed 10 test runs to detect inconsistent test behavior." >> flaky-report.md
        echo "" >> flaky-report.md

        # Simple analysis of failed tests across runs
        failed_tests=$(cat reports/flaky-run-*.json | jq -r '.tests[] | select(.outcome == "failed") | .nodeid' | sort | uniq -c | sort -nr)

        if [ -n "$failed_tests" ]; then
          echo "## Potentially Flaky Tests" >> flaky-report.md
          echo "\`\`\`" >> flaky-report.md
          echo "$failed_tests" >> flaky-report.md
          echo "\`\`\`" >> flaky-report.md
        else
          echo "## âœ… No Flaky Tests Detected" >> flaky-report.md
          echo "All tests showed consistent behavior across 10 runs." >> flaky-report.md
        fi

    - name: Upload flaky test report
      uses: actions/upload-artifact@v3
      with:
        name: flaky-test-report
        path: flaky-report.md