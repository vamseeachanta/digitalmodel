name: Optimized Test Suite

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      run_performance_tests:
        description: 'Run performance tests'
        required: false
        default: 'false'
        type: boolean
      test_selection:
        description: 'Test selection strategy'
        required: false
        default: 'smart'
        type: choice
        options:
          - 'all'
          - 'smart'
          - 'changed'

env:
  PYTHON_VERSION: '3.11'
  UV_CACHE_DIR: ~/.cache/uv
  FORCE_COLOR: '1'
  PYTEST_ADDOPTS: '--color=yes'

jobs:
  # Smart test selection based on changed files
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      core-changed: ${{ steps.changes.outputs.core }}
      modules-changed: ${{ steps.changes.outputs.modules }}
      tests-changed: ${{ steps.changes.outputs.tests }}
      docs-changed: ${{ steps.changes.outputs.docs }}
      config-changed: ${{ steps.changes.outputs.config }}
      test-selection: ${{ steps.test-selection.outputs.strategy }}
      affected-modules: ${{ steps.modules.outputs.list }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: dorny/paths-filter@v2
        id: changes
        with:
          filters: |
            core:
              - 'src/digitalmodel/**'
              - 'digitalmodel/**'
            modules:
              - 'src/digitalmodel/modules/**'
            tests:
              - 'tests/**'
            docs:
              - 'docs/**'
              - 'README.md'
            config:
              - 'pyproject.toml'
              - 'uv.lock'
              - '.github/workflows/**'

      - name: Determine test selection strategy
        id: test-selection
        run: |
          if [[ "${{ github.event.inputs.test_selection }}" == "all" ]]; then
            echo "strategy=all" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_selection }}" == "changed" ]]; then
            echo "strategy=changed" >> $GITHUB_OUTPUT
          elif [[ "${{ steps.changes.outputs.core }}" == "true" || "${{ steps.changes.outputs.config }}" == "true" ]]; then
            echo "strategy=all" >> $GITHUB_OUTPUT
          else
            echo "strategy=smart" >> $GITHUB_OUTPUT
          fi

      - name: Identify affected modules
        id: modules
        if: steps.test-selection.outputs.strategy != 'all'
        run: |
          affected_modules=""
          if [[ "${{ steps.changes.outputs.modules }}" == "true" ]]; then
            # Get list of changed module directories
            changed_files=$(git diff --name-only HEAD~1 HEAD -- 'src/digitalmodel/modules/**' || echo "")
            for file in $changed_files; do
              if [[ $file =~ src/digitalmodel/modules/([^/]+) ]]; then
                module="${BASH_REMATCH[1]}"
                if [[ ! " $affected_modules " =~ " $module " ]]; then
                  affected_modules="$affected_modules $module"
                fi
              fi
            done
          fi
          echo "list=${affected_modules}" >> $GITHUB_OUTPUT

  # Code quality and static analysis
  quality-checks:
    name: Quality Checks
    runs-on: ubuntu-latest
    needs: detect-changes
    strategy:
      matrix:
        check: [ruff, black, isort, mypy]
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Cache quality tools
        uses: actions/cache@v4
        with:
          path: ~/.cache/pre-commit
          key: pre-commit-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml') }}
          restore-keys: |
            pre-commit-${{ runner.os }}-

      - name: Install dependencies
        run: |
          uv sync --frozen --no-dev
          uv pip install ruff black isort mypy pre-commit

      - name: Run ${{ matrix.check }}
        run: |
          case "${{ matrix.check }}" in
            ruff)
              uv run ruff check . --output-format=github
              ;;
            black)
              uv run black --check --diff .
              ;;
            isort)
              uv run isort --check-only --diff .
              ;;
            mypy)
              uv run mypy src/ --ignore-missing-imports --show-error-codes
              ;;
          esac

  # Unit tests with smart parallelization
  unit-tests:
    name: Unit Tests
    runs-on: ${{ matrix.os }}
    needs: [detect-changes, quality-checks]
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ['3.9', '3.10', '3.11']
        exclude:
          # Reduce matrix size for non-main branches
          - os: windows-latest
            python-version: '3.9'
          - os: windows-latest
            python-version: '3.10'

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python ${{ matrix.python-version }}
        run: uv python install ${{ matrix.python-version }}

      - name: Cache test results
        uses: actions/cache@v4
        with:
          path: |
            .pytest_cache
            .mypy_cache
            .coverage
          key: test-cache-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('uv.lock') }}-${{ hashFiles('tests/**/*.py') }}
          restore-keys: |
            test-cache-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('uv.lock') }}-
            test-cache-${{ runner.os }}-${{ matrix.python-version }}-

      - name: Install dependencies
        run: |
          uv sync --frozen
          uv pip install pytest-xdist pytest-cov pytest-mock pytest-benchmark coverage[toml]

      - name: Install system dependencies (Ubuntu)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y libhdf5-dev libnetcdf-dev

      - name: Run tests (All)
        if: needs.detect-changes.outputs.test-selection == 'all'
        run: |
          uv run pytest -n auto \
            --cov=src \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-report=html \
            --cov-fail-under=85 \
            --junitxml=test-results.xml \
            --tb=short \
            tests/

      - name: Run tests (Smart selection)
        if: needs.detect-changes.outputs.test-selection == 'smart' && needs.detect-changes.outputs.affected-modules != ''
        run: |
          modules="${{ needs.detect-changes.outputs.affected-modules }}"
          test_paths=""
          for module in $modules; do
            if [ -d "tests/modules/$module" ]; then
              test_paths="$test_paths tests/modules/$module"
            fi
          done

          if [ -n "$test_paths" ]; then
            uv run pytest -n auto \
              --cov=src \
              --cov-report=xml \
              --cov-report=term-missing \
              --junitxml=test-results.xml \
              --tb=short \
              $test_paths
          else
            echo "No specific tests to run for changed modules"
            uv run pytest -n auto \
              --cov=src \
              --cov-report=xml \
              --maxfail=5 \
              tests/test_basic.py || true
          fi

      - name: Run tests (Changed files only)
        if: needs.detect-changes.outputs.test-selection == 'changed'
        run: |
          # Run only tests that correspond to changed files
          changed_test_files=""
          if [[ "${{ needs.detect-changes.outputs.tests-changed }}" == "true" ]]; then
            changed_test_files=$(git diff --name-only HEAD~1 HEAD -- 'tests/**/*.py' | tr '\n' ' ')
          fi

          if [ -n "$changed_test_files" ]; then
            uv run pytest -n auto \
              --cov=src \
              --cov-report=xml \
              --junitxml=test-results.xml \
              $changed_test_files
          else
            echo "No test files changed, running smoke tests"
            uv run pytest -n auto --maxfail=3 -k "not slow" tests/
          fi

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
          path: |
            test-results.xml
            htmlcov/
            .coverage

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-${{ matrix.os }}-${{ matrix.python-version }}
          fail_ci_if_error: false

  # Integration tests with module isolation
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [detect-changes, unit-tests]
    if: needs.detect-changes.outputs.core-changed == 'true' || needs.detect-changes.outputs.test-selection == 'all'
    strategy:
      matrix:
        module: [orcaflex, aqwa, fatigue_analysis, mcp]
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          uv sync --frozen
          uv pip install pytest-xdist pytest-timeout

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libhdf5-dev libnetcdf-dev

      - name: Run integration tests for ${{ matrix.module }}
        run: |
          if [ -d "tests/modules/${{ matrix.module }}" ]; then
            uv run pytest \
              --timeout=300 \
              --tb=short \
              -v \
              tests/modules/${{ matrix.module }}/test_*integration*.py || true
          else
            echo "No integration tests found for ${{ matrix.module }}"
          fi

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-results-${{ matrix.module }}
          path: test-results-integration.xml

  # Performance regression testing
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [detect-changes, unit-tests]
    if: github.event.inputs.run_performance_tests == 'true' || (github.event_name == 'push' && github.ref == 'refs/heads/main')
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          uv sync --frozen
          uv pip install pytest-benchmark memory-profiler psutil

      - name: Cache benchmark history
        uses: actions/cache@v4
        with:
          path: .benchmarks
          key: benchmark-${{ runner.os }}-${{ hashFiles('tests/performance/**/*.py') }}
          restore-keys: |
            benchmark-${{ runner.os }}-

      - name: Run performance benchmarks
        run: |
          uv run pytest \
            --benchmark-only \
            --benchmark-autosave \
            --benchmark-compare-fail=mean:20% \
            tests/performance/ || true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: .benchmarks/

      - name: Performance regression check
        run: |
          if [ -f ".benchmarks/Linux-CPython-3.11-64bit/0001_*.json" ]; then
            echo "Benchmark results available for regression analysis"
            # Compare with baseline (implement regression detection logic)
            uv run python scripts/check_performance_regression.py || true
          fi

  # Documentation and coverage verification
  documentation-check:
    name: Documentation Check
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.docs-changed == 'true' || needs.detect-changes.outputs.core-changed == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install documentation dependencies
        run: |
          uv sync --frozen
          uv pip install sphinx sphinx-rtd-theme sphinx-autodoc-typehints

      - name: Check documentation coverage
        run: |
          uv run python scripts/check_doc_coverage.py src/ || true

      - name: Build documentation
        run: |
          cd docs && make html || true

      - name: Upload documentation
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: documentation
          path: docs/_build/html/

  # Test summary and reporting
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests, documentation-check]
    if: always()
    steps:
      - uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-artifacts/

      - name: Generate test summary
        run: |
          echo "# Test Summary" > test-summary.md
          echo "" >> test-summary.md
          echo "## Test Results" >> test-summary.md

          # Count test files
          find test-artifacts/ -name "test-results*.xml" | wc -l > test-count.txt
          echo "- Total test result files: $(cat test-count.txt)" >> test-summary.md

          # Check for benchmark results
          if [ -d "test-artifacts/benchmark-results" ]; then
            echo "- Performance benchmarks: ✅ Available" >> test-summary.md
          else
            echo "- Performance benchmarks: ⏭️ Skipped" >> test-summary.md
          fi

          # Check for documentation
          if [ -d "test-artifacts/documentation" ]; then
            echo "- Documentation: ✅ Generated" >> test-summary.md
          else
            echo "- Documentation: ⏭️ Not required" >> test-summary.md
          fi

          echo "" >> test-summary.md
          echo "## Coverage Information" >> test-summary.md
          echo "Detailed coverage reports are available in the artifacts." >> test-summary.md

      - name: Upload test summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: test-summary.md

      - name: Post summary to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('test-summary.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

  # Cache cleanup for long-running branches
  cache-cleanup:
    name: Cache Cleanup
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - name: Cleanup old caches
        uses: actions/github-script@v7
        with:
          script: |
            const { data: caches } = await github.rest.actions.getActionsCaches({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 100
            });

            const oneWeekAgo = new Date(Date.now() - 7 * 24 * 60 * 60 * 1000);

            for (const cache of caches.actions_caches) {
              const cacheDate = new Date(cache.created_at);
              if (cacheDate < oneWeekAgo && !cache.key.includes('main')) {
                try {
                  await github.rest.actions.deleteActionsCacheById({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    cache_id: cache.id
                  });
                  console.log(`Deleted cache: ${cache.key}`);
                } catch (error) {
                  console.log(`Failed to delete cache ${cache.key}: ${error.message}`);
                }
              }
            }